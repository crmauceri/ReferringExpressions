# ObjectRef

1. [Data](#data)
    1. [Data splits](#data-splits)
    1. [Data preprocessing](#data-preprocessing)
2. [Modules](#modules)
3. [Classifiers](#classifiers)
    1. [Training Arguments](#training-arguments)
4. [Analysis and Visualizations](#analysis-and-visualization)

## Data
### Data Splits

`Stratifier` will split the images into a training and test set. It builds a binary tree which splits on the most frequent class label at each level. It terminates the branching when there are five or less images on a branch. It then randomly splits each leaf into training and test examples by a given percentage.

The purpose of `Stratifier` is to force the distribution of labels in the training and test sets to be as similar as possible. Because many images have multiple labels, `Stratifier` also attempts to balance the images with similar groups of labels. Because of the long tail of labels, `Stratifier` cares more about frequent labels than infrequent labels.

`Stratifier` uses the same JSON files as the Data Preprocessing step below, and outputs text files which list the image ids for each split. These files are optional input for the DataLoader.

Example Call

    python3 code/Stratifier.py <answer_file.json> <label_file.txt> <save_filepath>

### Data Preprocessing

`DataLoader` will preprocess data in correctly formatted JSON files to be compatible with the classifiers

The JSON format is 

    queries_file
    A list with, for each image:
	    A list of queries or a single query with fields:
		    ‘image_id’ - the image id
		    ‘tokenized’ - the tokenized query, the query word should be replaced with the <QUERY> token
		    ‘pos’ - the parts of speech for each token

    answers_file
    A list with, for each image:	
	    A list of answers or a single answer with fields:
		    ‘image_id’ - the image id
		    ‘multiple_choice_answer’ - the class labels

The lists must have the same order.

If you have a parse tree instead of part of speech tags, additional options on DataLoader will extract the part of speech tags from the parse tree. One additional field is required for the queries_file, ‘tree_idx’ which references the position of the corresponding parse tree in the tree_file


## Modules

1. `TruncatedImageNetworks` : torch.vision networks with the fully connected layers removed
2. `AttentionModule` : attention mechanism described in Z. Yang, X. He, J. Gao et al. "Stacked Attention Networks for Image Question Answering”. CVPR 2015.
3. `ClassifierHelper.Classifier` : classifiers inherit from this class; it provides training and test protocol.

## Classifiers

1. `SimpleClassifier_with_Tokens`: Classify missing words with LSTM.
2. `SimpleClassifier_with_Image`: Classify missing words with concatenated LSTM and VGG16 features.
3. `StackedAttentionNetwork.SAN`: Stacked attention network as described in Z. Yang, X. He, J. Gao et al. "Stacked Attention Networks for Image Question Answering”. CVPR 2015.

### Training Arguments

positional arguments| def
--- | ---
   data_file  |           Pickle file generated by DataLoader
   checkpoint_file  |     Filepath to save/load checkpoint. If file exists, checkpoint will be loaded
   
optional arguments | def
--- | ---
  -h, --help | show this help message and exit
  --epochs EPOCHS | Number of epochs to train (Default: 1)
  --hidden_dim HIDDEN_DIM |Size of LSTM embedding (Default:100)
  --use_tokens USE_TOKENS |If false, ignores pos token features. (Default:True)
  --tag_dim TAG_DIM | Size of tag embedding. If <1, will use one-hot representation (Default:10)

Notes:
- Shuffles examples between each epoch
- Does backprop on one example at a time, no mini-batching
- Record the options used for hidden_dim, use_tokens, and tag_dim. They are needed to load a saved model or analyze the results. 

## Analysis and Visualizations

`VisualizeResults` will generate three files

1. report - a per class precision/recall report
2. predictions - the predictions and score for every instance
3. confusion - a confusion matrix

Additionally, the model has a saved variable ‘total_error’ which records the total error per epoch. This can be used to analyze convergence. 
