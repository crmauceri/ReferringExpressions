import random, os, pickle, argparse
from tqdm import *

import torch
import torch.autograd as autograd
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.manual_seed(1)

from DataLoader import DataLoader
from DataLoader import Instance
from ClassifierHelper import Classifier

#Network Definition
class LSTM(nn.Module):

    def __init__(self, word_embedding, tag_dim, hidden_dim, output_dim,
                 tag_embedding_dim=10, use_tokens=True, use_cuda=False):
        super(LSTM, self).__init__()
        self.hidden_dim = hidden_dim
        self.tag_embedding_dim = tag_embedding_dim
        self.output_dim = output_dim

        self.use_tokens = use_tokens
        self.use_cuda = use_cuda

        self.word_embeddings = word_embedding
        embedding_dim = word_embedding.weight.size()[1]

        # The LSTM takes word embeddings as inputs, and outputs hidden states with dimensionality hidden_dim
        # These conditionals determine whether to use additional tag features as inputs and how to represent them
        if(use_tokens):
            if tag_embedding_dim > 0:
                #Use an embedded tag feature
                self.tag_embedding = nn.Embedding(tag_dim, tag_embedding_dim)
                self.lstm = nn.LSTM(embedding_dim + tag_embedding_dim + 1, hidden_dim)
            else:
                #Use a one-hot tag feature
                self.lstm = nn.LSTM(embedding_dim + tag_dim + 1, hidden_dim)
        else:
            #No tag features
            self.lstm = nn.LSTM(embedding_dim + 1, hidden_dim)

        if self.use_cuda:
            self.cuda()

    def init_hidden(self):
        # The axes semantics are (num_layers, minibatch_size, hidden_dim)
        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),
                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))

    def forward(self, sentence, query_flags, tags):
        if self.use_cuda:
            sentence = sentence.cuda()
            query_flags = query_flags.cuda()
            tags = tags.cuda()

        embeds = self.word_embeddings(sentence)
        if self.use_tokens:
            if self.tag_embedding_dim > 0:
                tag_embeds = self.tag_embedding(autograd.Variable(tags.data.nonzero()[:,1]))
            else:
                tag_embeds = tags
            x = torch.cat([embeds, tag_embeds, query_flags], 1)
        else:
            x = torch.cat([embeds, query_flags], 1)

        token_representation = x.view(len(sentence), 1, -1)
        out = self.lstm(token_representation)
        return out

class LSTMClassifier(Classifier):

    def __init__(self, word_embedding, tag_dim, hidden_dim, output_dim,
                 tag_embedding_dim=10, use_tokens=True, use_cuda=False):
        super(LSTMClassifier, self).__init__(use_cuda)
        self.output_dim = output_dim
        self.lstm = LSTM( word_embedding, tag_dim, hidden_dim, hidden_dim,
                 tag_embedding_dim=10, use_tokens=True, use_cuda=use_cuda)

        # The linear layer that maps from hidden state space to label space
        if self.use_cuda:
            self.outputLayer = nn.Sequential(nn.Linear(hidden_dim, output_dim).cuda(), nn.LogSoftmax())
            self.outputLayer = self.outputLayer.cuda()
            self.cuda()
        else:
            self.outputLayer = nn.Sequential(nn.Linear(hidden_dim, output_dim), nn.LogSoftmax())

    def forward(self, instance, parameters=None):
        lstm_out, lstm_hidden = self.lstm(instance.inputs, instance.flags, instance.pos_variable)
        return self.outputLayer(lstm_out[-1])


if __name__ == "__main__":

    parser = argparse.ArgumentParser(description='Classify missing words with LSTM.')
    parser.add_argument('data_file', help='Pickle file generated by DataLoader')
    parser.add_argument('checkpoint_file', help='Filepath to save/load checkpoint. If file exists, checkpoint will be loaded')
    parser.add_argument('--epochs', dest='epochs', type=int, default=1,
                        help='Number of epochs to train (Default: 1)')
    parser.add_argument('--hidden_dim', dest='hidden_dim', type=int, default=100,
                        help='Size of LSTM embedding (Default:100)')
    parser.add_argument('--use_tokens', dest='use_tokens', type=bool, default=True,
                        help='If false, ignores pos token features. (Default:True)')
    parser.add_argument('--tag_dim', dest='tag_dim', type=int, default=10,
                        help='Size of tag embedding. If <1, will use one-hot representation (Default:10)')
    args = parser.parse_args()

    with open(args.data_file, 'rb') as f:
        data = pickle.load(f)

    use_cuda = torch.cuda.is_available()
    model = LSTMClassifier(data.embed, len(data.tags_to_idx), args.hidden_dim, len(data.labels),
                           tag_embedding_dim=args.tag_dim, use_tokens=args.use_tokens, use_cuda=use_cuda)
    print("Start Training")

    total_loss = model.train(args.epochs, data.instances, args.checkpoint_file)